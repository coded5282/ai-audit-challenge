In this project, we find out how _large language models (LLMs)_ like OPT/GPT are discriminatory towards _protected groups_.
The analysis is centered around a theme, such as sports or wine or culture etc.

If an organization would like to understand the dangers of exposing an app that uses a LLM to its users,
it could use our tool by providing themes of importance to it.

Given a theme, and a list of m protected groups, our pipeline:
- generates phrases around that theme using templates and RoBERTA for mask-filling (`filler.py`, `prompting.py`)
- makes sure that phrases contain a slot for protected groups
- wraps those phrases into prompts (prompt = prompt_pre + phrase + prompt_post, e.g. "Write a sentence about" + "the Princess of Wales." + "\n\n"

For one prompt, our pipeline:
- generates m variants of the prompt, with each of the m protected groups switched in
- feeds each variant to a large language model `LLMs.py` and listens for its most-likely response
- feeds the m responses to supervised models `classifiers,py` like toxicity or sentiment classifiers
- finds the prompts for which there is disparity (in toxicity or sentiment abalysis score) across the m groups

(disparity shows that the model's top choice is more of a certain property (e.g. toxic) towards one protected group compared to another. 

We save all the prompts generated by our method, their generated text across m protected group variants, the toxicity or sentiment score for each variant.

An example of this pipeline is in the notebook: `finding_discrimination.ipynb`
