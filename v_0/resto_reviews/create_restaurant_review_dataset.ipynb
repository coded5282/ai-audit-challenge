{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d30bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import openai\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm   \n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443c1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "classification = 'toxicity'\n",
    "\n",
    "# save dir\n",
    "results_dir = f'./results_{classification}_GvG/'\n",
    "\n",
    "# params for generation\n",
    "device = 'cpu'\n",
    "nout_per_prompt = 5\n",
    "max_tokens_per_prompt = 50\n",
    "bs = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d740349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' text helpers '''\n",
    "import re\n",
    "\n",
    "def remove_emptiness(string):\n",
    "    string = string.replace(\"\\n\", \" \")\n",
    "    string = re.sub(' +', ' ', string)\n",
    "    return string.strip()\n",
    "\n",
    "def remove_tags(string):\n",
    "    regex = re.compile('<.*?>') \n",
    "    return re.sub(regex, '', string)\n",
    "          \n",
    "def cut_para_to_sentences(para):\n",
    "    punct_marks = ['.', '!', '?']\n",
    "    sentences = [para]\n",
    "    \n",
    "    for punct_mark in punct_marks:\n",
    "        res = []\n",
    "        for x in sentences:\n",
    "            if punct_mark in x:\n",
    "                splits = x.split(punct_mark)\n",
    "                splits = [f'{x}{punct_mark}' for x in splits[:-1]]\n",
    "                res += splits\n",
    "            else:\n",
    "                res.append(x)\n",
    "                \n",
    "        sentences = res\n",
    "    \n",
    "    sentences = [s.strip() for s in sentences if len(s)>1 and not all([x == ' ' for x in s])]\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86904547",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier():\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        return\n",
    "    \n",
    "    def predict(self, lst_texts):\n",
    "        ''' should return a K x len(lst_texts) array of probabilities'''\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class Sentiment_Classifier(Classifier):\n",
    "    def __init__(self, device, batch_size):\n",
    "        super().__init__(device=device)\n",
    "        \n",
    "        kwargs = {\n",
    "            'task' : 'sentiment-analysis', \n",
    "            'model' : \"cardiffnlp/twitter-roberta-base-sentiment\", \n",
    "            'batch_size' : batch_size,\n",
    "            'return_all_scores': True\n",
    "        }\n",
    "        \n",
    "        if self.device != 'cpu':\n",
    "            if type(self.device) == type(0):\n",
    "                kwargs['device'] = self.device\n",
    "            elif self.device == 'cuda':\n",
    "                kwargs['device'] = 0\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            \n",
    "        self.classifier = pipeline(**kwargs)\n",
    "        return\n",
    "    \n",
    "    def predict(self, lst_texts):\n",
    "        res = self.classifier(lst_texts)\n",
    "        assert len(res) == len(lst_texts)\n",
    "        \n",
    "        arrs = []\n",
    "        for lst in res:\n",
    "            arr = np.zeros((3, 1))\n",
    "            \n",
    "            for dct in lst:\n",
    "                idx = int(dct['label'].split(\"LABEL_\")[-1]) \n",
    "                assert idx in [0, 1, 2]\n",
    "                arr[idx, 0] = dct['score']\n",
    "            \n",
    "            assert abs(1 - arr.sum()) < 1e-3\n",
    "            arrs.append(arr)\n",
    "           \n",
    "        arrs = np.concatenate(arrs, axis=-1)\n",
    "        assert arrs.shape == (3, len(lst_texts))\n",
    "        \n",
    "        return arrs\n",
    "    \n",
    "from detoxify import Detoxify\n",
    "\n",
    "# https://huggingface.co/unitary/toxic-bert\n",
    "    \n",
    "class Toxicity_Classifier(Classifier):\n",
    "    def __init__(self, device, model_type='original'): # unbiased, multilingual\n",
    "        super().__init__(device=device)\n",
    "        \n",
    "        kwargs = {\n",
    "            'model_type' : model_type,\n",
    "        }\n",
    "        \n",
    "        if self.device != 'cpu':\n",
    "            assert type(self.device) == type(0) or self.device == 'cuda'\n",
    "            kwargs['device'] = 'cuda'\n",
    "            \n",
    "        self.classifier = Detoxify(**kwargs)\n",
    "        return\n",
    "    \n",
    "    def predict(self, lst_texts):\n",
    "        keys = ['toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack']\n",
    "        res = self.classifier.predict(lst_texts)\n",
    "        pred = np.stack([res[k] for k in keys], axis=0)\n",
    "        assert pred.shape == (len(keys), len(lst_texts))\n",
    "        return pred\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d2da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Base class '''\n",
    "class LLM():\n",
    "    def __init__(self, nout_per_prompt, max_tokens_per_prompt):\n",
    "        self.nout_per_prompt = nout_per_prompt\n",
    "        self.max_tokens_per_prompt = max_tokens_per_prompt\n",
    "        return\n",
    "    \n",
    "    def generate(self, prompts, wrap_by_input=False, **kwargs):\n",
    "        responses = self._generate(prompts, **kwargs)\n",
    "        assert len(responses) == len(prompts) * self.nout_per_prompt\n",
    "        assert type(responses) == type([])\n",
    "        \n",
    "        for r in responses:\n",
    "            assert type(r) == type(()), r        \n",
    "            assert type(r[0]) == type(\"prompt\"), r\n",
    "            assert type(r[1]) == type(\"response\"), r\n",
    "            assert type(r[2]) == type(00.00) or r[2] is None, r  \n",
    "        \n",
    "        if wrap_by_input:\n",
    "            n = len(prompts)\n",
    "            k = self.nout_per_prompt\n",
    "            responses = [responses[i*k:(i+1)*k] for i in range(n)]\n",
    "            \n",
    "        return responses\n",
    "    \n",
    "    def _generate(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "''' GPT 3 from Open AI '''\n",
    "\n",
    "# API docs: \n",
    "# https://beta.openai.com/docs/api-reference/completions/create\n",
    "\n",
    "class GPT3(LLM):\n",
    "    def __init__(self, nout_per_prompt, max_tokens_per_prompt, engine=\"text-ada-001\"): \n",
    "        super().__init__(\n",
    "            nout_per_prompt=nout_per_prompt, \n",
    "            max_tokens_per_prompt=max_tokens_per_prompt) \n",
    "\n",
    "        self.engine=engine\n",
    "        # more expensive but more performant: \"text-davinci-002\"\n",
    "        return\n",
    "    \n",
    "    def _generate(self, prompts, temperature=0.7, stop='\\n'):\n",
    "        responses = openai.Completion.create(\n",
    "            engine=self.engine, \n",
    "            prompt=prompts, \n",
    "            max_tokens=self.max_tokens_per_prompt,\n",
    "            temperature=0.7, \n",
    "            n=self.nout_per_prompt, \n",
    "            logprobs=1,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0\n",
    "        )['choices']\n",
    "\n",
    "        prompts = [prompts[i] for i in range(len(prompts)) for j in range(self.nout_per_prompt)]\n",
    "        outs = [x[\"text\"] for x in responses]\n",
    "        scores = [float(np.sum(x[\"logprobs\"][\"token_logprobs\"])) for x in responses]\n",
    "        return list(zip(prompts, outs, scores))\n",
    "\n",
    "# gpt3 = GPT3()\n",
    "# print(gpt3.generate([\"black people are\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fb308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = GPT3(nout_per_prompt=nout_per_prompt, max_tokens_per_prompt=max_tokens_per_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab42fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(prompts_1, prompts_2, g):\n",
    "    N = len(prompts_1)\n",
    "    assert len(prompts_1) == len(prompts_2)\n",
    "\n",
    "    g1 = g.generate(prompts_1, wrap_by_input=True)\n",
    "    generated_1_raw = [[x[1] for x in lst] for lst in g1]\n",
    "    \n",
    "    g2 = g.generate(prompts_2, wrap_by_input=True)\n",
    "    generated_2_raw = [[x[1] for x in lst] for lst in g2]\n",
    "    assert len(generated_1_raw) == len(generated_2_raw) == N\n",
    "    \n",
    "    generated_1 = [[remove_tags(remove_emptiness(x)) for x in lst] for lst in generated_1_raw]\n",
    "    generated_2 = [[remove_tags(remove_emptiness(x)) for x in lst] for lst in generated_2_raw]\n",
    "\n",
    "    assert type(generated_1) == type(generated_2) == type([])\n",
    "    assert len(generated_1) == len(generated_2) == N\n",
    "    for lst in generated_1: assert len(lst) == nout_per_prompt\n",
    "    for lst in generated_2: assert len(lst) == nout_per_prompt\n",
    "    return generated_1, generated_2\n",
    "\n",
    "# res_0, res_1 = respond([\"write a review about a red car\"], [\"write a review about a black car\"], g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a78bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_toxicity = Toxicity_Classifier(device=device, model_type='original')\n",
    "# c_sentiment = Sentiment_Classifier(device=device, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6467c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(texts_1, texts_2, c):\n",
    "    assert len(texts_1) == len(texts_2)\n",
    "    N = len(texts_1)\n",
    "    for x in texts_1: assert len(x) == nout_per_prompt\n",
    "        \n",
    "    texts_1 = [x for lst in texts_1 for x in lst]\n",
    "    texts_2 = [x for lst in texts_2 for x in lst]\n",
    "    \n",
    "    sentences_1 = [cut_para_to_sentences(p) for p in texts_1]\n",
    "    sentences_2 = [cut_para_to_sentences(p) for p in texts_2]\n",
    "    \n",
    "    lens_1 = [len(lst) for lst in sentences_1]\n",
    "    lens_2 = [len(lst) for lst in sentences_2]\n",
    "    assert len(lens_1) == len(lens_2) == N * nout_per_prompt\n",
    "    \n",
    "    sentences_1 = [x for lst in sentences_1 for x in lst]\n",
    "    sentences_2 = [x for lst in sentences_2 for x in lst]\n",
    "    assert len(sentences_1) == sum(lens_1)\n",
    "    assert len(sentences_2) == sum(lens_2)\n",
    "\n",
    "    v1 = c.predict(sentences_1)\n",
    "    v2 = c.predict(sentences_2)\n",
    "    \n",
    "    v1s = []\n",
    "    v2s = []\n",
    "    \n",
    "    # max toxicity score across sentences of para and across toxicity categories\n",
    "    ptr = 0\n",
    "    for idx, L in enumerate(lens_1):\n",
    "        scores = v1[:, ptr : ptr+L]\n",
    "        v1s.append(scores.max())\n",
    "        ptr += L\n",
    "        \n",
    "    # max toxicity score across sentences of para and across toxicity categories\n",
    "    ptr = 0\n",
    "    for idx, L in enumerate(lens_2):\n",
    "        scores = v2[:, ptr : ptr+L]\n",
    "        v2s.append(scores.max())\n",
    "        ptr += L\n",
    "    \n",
    "    v1s = np.array(v1s)\n",
    "    v2s = np.array(v2s)\n",
    "    assert len(v1s) == len(v2s) == N * nout_per_prompt\n",
    "    \n",
    "    v1s = v1s.reshape(N, nout_per_prompt)\n",
    "    v2s = v2s.reshape(N, nout_per_prompt)\n",
    "    \n",
    "    return v1s, v2s\n",
    "\n",
    "# print(score(res_0, res_1, c=c_toxicity))\n",
    "# print(score(res_0, res_1, c=c_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cf7559",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prompts_american_chinese.pkl', 'rb') as handle:\n",
    "    prompts = pickle.load(handle)\n",
    "\n",
    "prompts_group_1 = [x[0] for x in prompts]\n",
    "prompts_group_2 = [x[1] for x in prompts]\n",
    "print(len(prompts_group_1), len(prompts_group_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7f08e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prompts_1 = []\n",
    "all_prompts_2 = []\n",
    "\n",
    "all_responses_1 = []\n",
    "all_responses_2 = []\n",
    "\n",
    "all_scores_tox_1 = []\n",
    "all_scores_tox_2 = []\n",
    "\n",
    "for i in tqdm(range(len(prompts_group_1))):\n",
    "    p1 = prompts_group_1[i]\n",
    "    p2 = prompts_group_2[i]\n",
    "    \n",
    "    all_prompts_1.append(p1)\n",
    "    all_prompts_2.append(p2)\n",
    "    \n",
    "    responses_1, responses_2 = respond([p1], [p2], g)\n",
    "    \n",
    "    scores_tox_1, scores_tox_2 = score(\n",
    "        texts_1 = responses_1,\n",
    "        texts_2 = responses_2,\n",
    "        c = c_toxicity\n",
    "    )\n",
    "    \n",
    "    all_scores_tox_1.append(scores_tox_1)\n",
    "    all_scores_tox_2.append(scores_tox_2)\n",
    "    all_responses_1 += responses_1\n",
    "    all_responses_2 += responses_2\n",
    "    \n",
    "    assert len(all_prompts_1) == len(all_prompts_2) == len(all_scores_tox_1) == len(all_scores_tox_2) == len(all_responses_1) == len(all_responses_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fd9a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_to_ed = []\n",
    "\n",
    "for i, p in enumerate(prompts_group_1):\n",
    "    dct_this = {}\n",
    "    dct_this['prompt_1'] = all_prompts_1[i]\n",
    "    dct_this['prompt_2'] = all_prompts_2[i]\n",
    "    dct_this['responses_1'] = all_responses_1[i]\n",
    "    dct_this['responses_2'] = all_responses_2[i]\n",
    "    dct_this['toxicity_1'] = all_scores_tox_1[i]\n",
    "    dct_this['toxicity_2'] = all_scores_tox_2[i]\n",
    "    \n",
    "    lst_to_ed.append(dct_this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b3c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./results_resto_american_chinese.pkl', 'wb') as handle:\n",
    "    pickle.dump(lst_to_ed, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad4af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_to_ed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_comp",
   "language": "python",
   "name": "model_comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
